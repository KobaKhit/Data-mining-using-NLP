
# coding: utf-8

# # Data mining New York Times articles
# by Harshini Konduri, Pallavi Damle, Shantanu Saha, Harshvardhan Shukla, Koba Khitalishvili
# 
# 
# ## Introduction
# In our project we explore Natural Language Processing by mining the New York Times articles. The basic idea was to see if it is possible to classify the articles by topic using the article abstracts. By classifying the articles we are able to obtain the most important words for each topic using the feature importances generated by the classifier. Potential use would be for marketing purposes to see what keywords are trending for a given topic. 
# 
# We are using open source tools like machine learning python library Scikit-learn and Beautiful soup for web scraping. We are proponents of reproducible research and you can find all our code at https://github.com/KobaKhit/Data-mining-using-NLP .
# 
# ## Results
# ### Data preparation
# Initially, we wrote a python script that scraped new york times articles for information like date published, author, and most importantly the body of text. However, some articles are private and cannot be scraped. Additionally, the webscraping was slow. As a result, we wrote a different script ,`nytsnippetgetter.py`, that sends direct http search request to the www.nytimes.com/ server and gets a `json` response instantly. Instead of the body of text we get a snippet which can be the first leading paragraph, a summary, or an abstract. We were able to obtain 36 thousand article abstracts and other information in less than 5 minutes. After preprocessing the raw text data and obtaining the TFxIDF statistics for each word across all articles we use the multinomial Naive Bayes model for classification. 
# 
# ### Classification
# You can see in appendix that our model has 100% accuracy on test data when classifying by article topic which basically means that the New York Times search engine is efficient. However, when classifying articles from different time periods by author the accuracy of the model is only 0.8. In every instance we provide the most important words by topic (author).
# 
# So far we found out that New York Times has a superb search engine and that authors from distinct time periods had different vocabularly which is sort of obvious. At this point we are continueing our reserch in several directions. 
# 
# We look at comparing word clouds at different time points and how vocabularly changed with time. We have access to articles dating as early as 1921. To do so we decided to use the Topic modelling by latent Dirichlet allocation. There is an implimentation of the algorithm for python called the `lda` package. However, scikit-learn has an implementation of the same algorithm as well ([sklearn.decomposition.LatentDirichletAllocation](http://scikit-learn.org/dev/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)). We were inspired by Blei's approach of topic modelling (2012).
# 
# We also want to test whether its possible  to classify articles obtained from different news sources using NY Times articles as training data. 
# 
# 
# ## Conclusion
# Going forward we want to focus on data preprocessing. Namely, edit the corpus of the articles and remove redundant words like prepositions. In this way we will be able to capture most important words by topic. According to Blei, efficient topic modeling increases the relevance and flexibility of any text data search results. 
# 
# Additionaly, we would like to make use of other classifiers like RandomForest and Support vector machines and maybe an ensemble method.
# 
# ## References 
#   - Blei, David. Probabilistic Topic Models. *Communications of the ACM*. April 2012. vol. 55. no. 4
#   
# ## Appendix

# In[2]:

from nytsnippetgetter import get_data
import time, os.path, json, re

# get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt

import numpy as np

# start timer
start_time = time.time()

# get available number of pages for each topic. Each page is equivalent to 10 articles
topics=['economics','politics','espionage','global+warming', 'donald+trump','hillary+clinton', 
        'bernie+sanders', 'guns', 'cancer', 'sex']
ndocs = [500,500,500,500,500,500,500,500,500,500]
# topics=['economics','politics']
get_data(topics, BEGINDATE = 20131213, LIMITS=True) # articles written since 2013-December-13


# In[3]:

if os.path.isfile("../scripts/2016-05-11-election-data.json"):
    # load saved data
    with open('../scripts/2016-05-11-election-data.json') as json_data:
        articles = json.load(json_data)["data"]
    print("Loaded " + str(len(articles)) + " articles.")
else:
    # download article data 
    # articles = get_data(TOPICS = topics, NPAGES = npages, BEGINDATE = 20131213)
    articles = get_data(topics, ndocs, BEGINDATE=20140101, FILENAME='example.json')
    # articles = get_data(TOPICS = topics, BEGINDATE = 20131213, NPAGES = [15,10])


# In[4]:

# articles is a list of objects with each object being one document
articles[0]


# In[5]:

# retain unique articles
unique_articles = []
data = [x['snippet'] for x in unique_articles]
for i in articles:
    if i['snippet'] not in data: 
        unique_articles.append(i)
        
# some articles are missing lead paragraph and abstract. just use snippets for now
data = [re.sub(r'(<.*>)','',x['snippet']) for x in unique_articles if x['user_topic'] in 
        ['donald+trump','hillary+clinton', 'bernie+sanders', 'ted+cruz']]
words = set([ y for x in unique_articles for y in x['snippet'].split(" ")])

# show html tags
[x for x in words if "<" in x][0:5]


# In[6]:

# number of articles
print("N articles: ", len(data))
print("N unique articles: ", len(set(data)))
print("N unique words: ", len(set([ y for x in data for y in x.split(" ")])))

# words
[y for x in data for y in x.split(" ")][0:5]


# In[7]:

# https://pypi.python.org/pypi/lda
# http://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi
# stoplist = set('for a of the and to in'.split())
# data = [[word for word in document.lower().split() if word not in stoplist]
#          for document in documents]

# check if data has Nones
nonesidx = [data.index(x) for x in data if x == None and len(x) > 0]
if len(nonesidx) > 0:
    print("You have nones. Below are indices of articles.")
    print(nonesidx)
else:
    print("No Nones. Good to go.")
    
data[0:3]


# In[8]:

# article topics
label = [ x['user_topic'] for x in unique_articles if x['user_topic'] in 
        ['donald+trump','hillary+clinton', 'bernie+sanders', 'ted+cruz']]
label[-5:-1]


# In[9]:

# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import preprocessing

# Convert raw documents to a matrix of TF-IDF features.
vectorizer = TfidfVectorizer(stop_words = "english", ngram_range =(1,2)
                            #,max_features = 50000
                             )

# Encode labels with value between 0 and n_classes-1.
le = preprocessing.LabelEncoder() 

vectors = vectorizer.fit_transform(data) 
target = le.fit_transform(label)

print(vectors.shape, target.shape,'\n')
print("Number of columns: ", vectors.shape[1], '\n')
print("(docid, wordid) TFIDF", '\n')
print(vectors[0,])


# In[10]:

# average number of non-zero components by sample, 
# i.e. average number of words with non-zero IDFxTF by article
vectors.nnz / float(vectors.shape[0])


# In[11]:

from sklearn.cross_validation import train_test_split
a, data_, b, target_ = train_test_split(vectors, target, test_size=0.1, random_state=1729)
a,b, data_ = 0,0, data_.todense()
data_.shape, target_.shape


# In[12]:

counts = np.bincount(target)
counts


# In[13]:

topics = ['donald+trump','hillary+clinton', 'bernie+sanders', 'ted+cruz']
plt.bar(list(set(target_)),counts, align='center')
plt.xticks(list(set(target_)),topics)
plt.title("Histogram of articles by topic")


# In[327]:

# plot first two principal components
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize

data_ = normalize(data_)

pca = PCA(n_components=2).fit(data_)
data2D = pca.transform(data_)

# plt.scatter(data2D[:,0], data2D[:,1], c=target_)

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(1, 1, 1)
colors = ['black','red','green','grey']
classes = list(set(target_))
for color, label, class_ix in zip(
        colors, topics, classes):
    ax.scatter(data2D[np.where(target_ == class_ix),0],
               data2D[np.where(target_ == class_ix),1],
               color=color, edgecolor='whitesmoke',
               linewidth='1', alpha=0.9, label=label)
    ax.legend(loc='best')
plt.title(
    "Scatter plot of the training data examples projected on the "
    "2 first principal components")
plt.xlabel("Principal axis 1 - Explains %.1f %% of the variance" % (
    pca.explained_variance_ratio_[0] * 100.0))
plt.ylabel("Principal axis 2 - Explains %.1f %% of the variance" % (
    pca.explained_variance_ratio_[1] * 100.0))
plt.show()


# In[14]:

X_train, X_test, y_train, y_test = train_test_split(vectors, target, 
                                                    test_size=0.2, random_state=1729)
print(X_train.shape, X_test.shape)


# In[15]:

from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import ExtraTreesClassifier
from sklearn import metrics

import pandas as pd

clf = MultinomialNB(alpha=1)
# clf = ExtraTreesClassifier(n_estimators = 25, criterion = "entropy", n_jobs=-1, 
#                            bootstrap=True, random_state=1729)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("F-score: ", metrics.f1_score(y_test, pred, average='weighted'))
print("Accuracy: ", metrics.precision_score(y_test, pred, average='weighted'))
print("Confusion matrix:\n", pd.crosstab(y_test, pred, rownames=['True'], 
                                         colnames=['Predicted']))


# In[17]:

def show_top10(classifier, vectorizer, categories):
    feature_names = np.asarray(vectorizer.get_feature_names())
    for i, category in enumerate(categories):
        top10 = np.argsort(classifier.coef_[i])[-10:]
        print("%s: %s" % (category, ",".join(np.unique(feature_names[top10])[0:20])))
        
# print ten most important words for each topic
show_top10(clf, vectorizer, le.classes_)


# In[18]:

# end timer
print(time.time()-start_time, 'seconds')


# In[176]:

# test the model on articles written before 2013-December-13
test = get_data(TOPICS = ['donald+trump','hillary+clinton', 'bernie+sanders', 'ted+cruz'],
                ENDDATE = 20151213, NDOCS = [20,20,20,20])
test_data = [ x['snippet'] for x in test]
test_label = [ x['user_topic'] for x in test]

test_vectors = vectorizer.transform(test_data) 
test_target = le.transform(test_label)

print(test_vectors.shape, test_target.shape,'\n')
print("Number of unique words (features) across all docs: ", vectors.shape[1], '\n')
print("(docid, wordid) TFIDF", '\n')
print(vectors[0,])


# In[177]:

pred_test = clf.predict(test_vectors)
print("F-score: ", metrics.f1_score(test_target, pred_test, average='weighted'))
print("Accuracy: ", metrics.precision_score(test_target, pred_test, average='weighted'))
print("Confusion matrix:\n", pd.crosstab(test_target, pred_test, 
                                         rownames=['True'], colnames=['Predicted']))


# In[ ]:

print("Homogeneity: %0.3f" % metrics.homogeneity_score(y_test, pred))
print("Completeness: %0.3f" % metrics.completeness_score(y_test, pred))
print("V-measure: %0.3f" % metrics.v_measure_score(y_test, pred))
print("Adjusted Rand-Index: %.3f"
      % metrics.adjusted_rand_score(y_test, pred))
print("Silhouette Coefficient: %0.3f"
      % metrics.silhouette_score(X_test, pred, sample_size=1000))


# ### Probabolistic Topic Modelling 
# #### Latent Dirichlet Allocation
# 
# Example is [here](http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html).

# In[29]:

from sklearn.decomposition import NMF, LatentDirichletAllocation

def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print("Topic #%d:" % topic_idx)
        print(", ".join([feature_names[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
    print()

n_top_words = 10

lda = LatentDirichletAllocation(n_topics=4, 
                                max_iter=5,
                                learning_method='online',
                                #learning_offset=50.,
                                random_state=1729)

lda.fit(vectors)

tf_feature_names = vectorizer.get_feature_names()
print_top_words(lda, tf_feature_names, n_top_words)


# #### Non-negative Matrix Factorization

# In[30]:

nmf = NMF(n_components=10, random_state=1,
          alpha=.01, l1_ratio=.5)

nmf.fit(vectors)
print_top_words(nmf, tf_feature_names, n_top_words)

