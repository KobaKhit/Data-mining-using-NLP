{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining New York Times articles\n",
    "by Harshini Konduri, Pallavi Damle, Shantanu Saha, Harshvardhan Shukla, Koba Khitalishvili\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In our project we explore Natural Language Processing by mining the New York Times articles. The basic idea was to see if it is possible to classify the articles by topic using the article abstracts. By classifying the articles we are able to obtain the most important words for each topic. Potential use would be for marketing purposes to see what keywords are trending for a given topic. We used the TFxIDF statistic for each unique word as a feature for our classification model.\n",
    "\n",
    "We are using open source tools like machine learning python library Scikit-learn and Beautiful soup for web scraping. We are proponents of reproducible research and you can find all our code at https://github.com/KobaKhit/Data-mining-using-NLP .\n",
    "\n",
    "## Results\n",
    "We wrote a python script that downloads data from the New York times. We were able to obtain 36 thousand article abstracts and other information in less than 5 minutes. After preprocessing the raw text data and obtaining the TFxIDF statistics for each word across all articles we use the multinomial Naive Bayes model for classification. \n",
    "\n",
    "You can see in appendix that our model has 100% accuracy on test data which basically means that the New York Times search engine is efficient. However, when classifying articles from different time periods by author the accuracy of the model is only 0.8. In every instance we provide the most important words by topic (author).\n",
    "\n",
    "## Conclusion\n",
    "Going forward we want to focus on data preprocessing. Namely, edit the corpus of the articles and remove redundant words like prepositions. In this way we will be able to capture the actual topic of the article. According to Blei, efficient topic modeling increases the relevance and flexibility of article or any text data search results. \n",
    "\n",
    "Additionaly, we would like to make use of other classifiers like RandomForest and Support vector machines.\n",
    "\n",
    "## References \n",
    "  - Blei, David. Probabilistic Topic Models. *Communications of the ACM*. April 2012. vol. 55. no. 4\n",
    "  \n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages available (each page is 10 articles): \n",
      "economics :  4584\n",
      "politics :  19196\n",
      "espionage :  1181\n",
      "global+warming :  2035\n",
      "clinton :  9229\n",
      "sanders :  3553\n",
      "guns :  3369\n",
      "cancer :  7645\n",
      "sex :  10003\n"
     ]
    }
   ],
   "source": [
    "from nytsnippetgetter import get_data\n",
    "import time\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# get available number of pages for each topic. Each page is equivalent to 10 articles\n",
    "topics=['economics','politics','espionage','global+warming', 'clinton', 'sanders', 'guns', \n",
    "        'cancer', 'sex']\n",
    "npages = [1500,1000,500,100,100,100,100,100,100]\n",
    "# topics=['economics','politics']\n",
    "get_data(topics, BEGINDATE = 20131213, LIMITS=True) # articles written since 2013-December-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:  ['economics', 'politics', 'espionage', 'global+warming', 'clinton', 'sanders', 'guns', 'cancer', 'sex']\n",
      "NPages:  [1500, 1000, 500, 100, 100, 100, 100, 100, 100] \n",
      "\n",
      "Total documents:  36000\n",
      "Started download...\n",
      "economics is done | 1500/3600\n",
      "politics is done | 2500/3600\n",
      "espionage is done | 3000/3600\n",
      "global+warming is done | 3100/3600\n",
      "clinton is done | 3200/3600\n",
      "sanders is done | 3300/3600\n",
      "guns is done | 3400/3600\n",
      "cancer is done | 3500/3600\n",
      "sex is done | 3600/3600\n",
      "\n",
      "Done in  457.22085785865784 seconds\n"
     ]
    }
   ],
   "source": [
    "# download article data \n",
    "articles = get_data(TOPICS = topics, NPAGES = npages, BEGINDATE = 20131213)\n",
    "# articles = get_data(TOPICS = topics, NPAGES = [150,100,50,10,10,10,10,10,10])\n",
    "# articles = get_data(TOPICS = topics, BEGINDATE = 20131213, NPAGES = [15,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Hating on anyone who suggests limits.',\n",
       " 'author': 'PAUL KRUGMAN',\n",
       " 'date_modified': '2016-04-22T16:55:06Z',\n",
       " 'date_published': '2016-04-22T10:54:17Z',\n",
       " 'keywords': [],\n",
       " 'lead_paragraph': None,\n",
       " 'nytclass': None,\n",
       " 'section_name': {'content': 'opinion', 'display_name': 'Opinion'},\n",
       " 'snippet': 'be doing anything especially new — on the contrary, he and his defenders claimed that they were doing standard Keynesian <strong>economics</strong> — apparently unaware that they were doing no such thing. Only after this was pointed out did they',\n",
       " 'title': 'Sarandonizing Economics',\n",
       " 'user_topic': 'economics',\n",
       " 'weburl': 'http://krugman.blogs.nytimes.com/2016/04/22/sarandonizing-economics/'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# articles is a list of objects with each object being one document\n",
    "articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of articles\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Nones. Good to go.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Psychology in economics',\n",
       " 'be doing anything especially new — on the contrary, he and his defenders claimed that they were doing standard Keynesian <strong>economics</strong> — apparently unaware that they were doing no such thing. Only after this was pointed out did they',\n",
       " 'GASOLINE prices on the South Fork are consistently 20 to 50 cents more per gallon than in other areas of Long Island. But the recent run-up in gasoline prices to record levels has been a rude awakening for some South Fork residents.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some articles are missing lead paragraph and abstract. just use snippets for now\n",
    "data = [ x['snippet'] for x in articles]\n",
    "\n",
    "# check if data has Nones\n",
    "nonesidx = [data.index(x) for x in data if x == None and len(x) > 0]\n",
    "if len(nonesidx) > 0:\n",
    "    print(\"You have nones. Below are indices of articles.\")\n",
    "    print(nonesidx)\n",
    "else:\n",
    "    print(\"No Nones. Good to go.\")\n",
    "    \n",
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'sex', 'sex', 'sex']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article topics\n",
    "label = [ x['user_topic'] for x in articles]\n",
    "label[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 1249) (36000,) \n",
      "\n",
      "Number of unique words (features) across all docs:  1249 \n",
      "\n",
      "(docid, wordid) TFIDF \n",
      "\n",
      "  (0, 376)\t0.423466656957\n",
      "  (0, 553)\t0.331808863831\n",
      "  (0, 889)\t0.84295840249\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "vectorizer = TfidfVectorizer() # Convert raw documents to a matrix of TF-IDF features.\n",
    "le = preprocessing.LabelEncoder() # Encode labels with value between 0 and n_classes-1.\n",
    "\n",
    "vectors = vectorizer.fit_transform(data) \n",
    "target = le.fit_transform(label)\n",
    "\n",
    "print(vectors.shape, target.shape,'\\n')\n",
    "print(\"Number of unique words (features) across all docs: \", vectors.shape[1], '\\n')\n",
    "print(\"(docid, wordid) TFIDF\", '\\n')\n",
    "print(vectors[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.18888888888889"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of non-zero components by sample, \n",
    "# i.e. average number of words with non-zero IDFxTF by article\n",
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24120, 1249) (11880, 1249)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, target, \n",
    "                                                    test_size=0.33, random_state=1729)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:  1.0\n",
      "Accuracy:  1.0\n",
      "Confusion matrix:\n",
      " Predicted    0    1     2     3    4    5     6    7    8\n",
      "True                                                     \n",
      "0          337    0     0     0    0    0     0    0    0\n",
      "1            0  321     0     0    0    0     0    0    0\n",
      "2            0    0  4910     0    0    0     0    0    0\n",
      "3            0    0     0  1670    0    0     0    0    0\n",
      "4            0    0     0     0  334    0     0    0    0\n",
      "5            0    0     0     0    0  343     0    0    0\n",
      "6            0    0     0     0    0    0  3327    0    0\n",
      "7            0    0     0     0    0    0     0  313    0\n",
      "8            0    0     0     0    0    0     0    0  325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "clf = MultinomialNB(alpha=2)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"F-score: \", metrics.f1_score(y_test, pred, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.precision_score(y_test, pred, average='weighted'))\n",
    "print(\"Confusion matrix:\\n\", pd.crosstab(y_test, pred, rownames=['True'], \n",
    "                                         colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer: with is and breast the of for as strong cancer\n",
      "clinton: trump art facility correctional mr his the in strong clinton\n",
      "economics: exposure and that of to psychology in the strong economics\n",
      "espionage: his and she to was in had the strong espionage\n",
      "global+warming: group is planet to climate the warming change strong global\n",
      "guns: night fort of increase at roses for strong the guns\n",
      "politics: may to province havana oriente arms by washington of the\n",
      "sanders: weight woman to year strong show is larry the sanders\n",
      "sex: that about its strangers of and in the strong sex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        \n",
    "# print ten most important words for each topic\n",
    "show_top10(clf, vectorizer, le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495.6035737991333 seconds\n"
     ]
    }
   ],
   "source": [
    "# end timer\n",
    "print(time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:  ['economics', 'politics', 'espionage', 'global+warming', 'clinton', 'sanders', 'guns', 'cancer', 'sex']\n",
      "NPages:  [10, 10, 10, 10, 10, 10, 10, 10, 10, 10] \n",
      "\n",
      "Total documents:  1000\n",
      "Started download...\n",
      "economics is done | 10/100\n",
      "politics is done | 20/100\n",
      "espionage is done | 30/100\n",
      "global+warming is done | 40/100\n",
      "clinton is done | 50/100\n",
      "sanders is done | 60/100\n",
      "guns is done | 70/100\n",
      "cancer is done | 80/100\n",
      "sex is done | 90/100\n",
      "\n",
      "Done in  13.58777117729187 seconds\n",
      "(900, 1249) (900,) \n",
      "\n",
      "Number of unique words (features) across all docs:  1249 \n",
      "\n",
      "(docid, wordid) TFIDF \n",
      "\n",
      "  (0, 376)\t0.423466656957\n",
      "  (0, 553)\t0.331808863831\n",
      "  (0, 889)\t0.84295840249\n"
     ]
    }
   ],
   "source": [
    "# test the model on articles written before 2013-December-13\n",
    "test = get_data(TOPICS = topics, ENDDATE = 20131213, NPAGES = [10,10,10,10,10,10,10,10,10,10])\n",
    "test_data = [ x['snippet'] for x in test]\n",
    "test_label = [ x['user_topic'] for x in test]\n",
    "\n",
    "test_vectors = vectorizer.transform(test_data) \n",
    "test_target = le.transform(test_label)\n",
    "\n",
    "print(test_vectors.shape, test_target.shape,'\\n')\n",
    "print(\"Number of unique words (features) across all docs: \", vectors.shape[1], '\\n')\n",
    "print(\"(docid, wordid) TFIDF\", '\\n')\n",
    "print(vectors[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:  1.0\n",
      "Accuracy:  1.0\n",
      "Confusion matrix:\n",
      " Predicted    0    1    2    3    4    5    6    7    8\n",
      "True                                                  \n",
      "0          100    0    0    0    0    0    0    0    0\n",
      "1            0  100    0    0    0    0    0    0    0\n",
      "2            0    0  100    0    0    0    0    0    0\n",
      "3            0    0    0  100    0    0    0    0    0\n",
      "4            0    0    0    0  100    0    0    0    0\n",
      "5            0    0    0    0    0  100    0    0    0\n",
      "6            0    0    0    0    0    0  100    0    0\n",
      "7            0    0    0    0    0    0    0  100    0\n",
      "8            0    0    0    0    0    0    0    0  100\n"
     ]
    }
   ],
   "source": [
    "pred_test = clf.predict(test_vectors)\n",
    "print(\"F-score: \", metrics.f1_score(test_target, pred_test, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.precision_score(test_target, pred_test, average='weighted'))\n",
    "print(\"Confusion matrix:\\n\", pd.crosstab(test_target, pred_test, \n",
    "                                         rownames=['True'], colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classify by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIMOTHY EGAN', 'KAREN ALEXANDER', 'DWIGHT GARNER', 'BEN KENIGSBERG']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article author\n",
    "label = [ x['author'] for x in articles]\n",
    "label[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24120, 1249) (11880, 1249)\n"
     ]
    }
   ],
   "source": [
    "target = le.fit_transform(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, target, test_size=0.33, \n",
    "                                                    random_state=1729)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:  0.815594555496\n",
      "Accuracy:  0.791768788424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tpot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/envs/tpot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"F-score: \", metrics.f1_score(y_test, pred, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.precision_score(y_test, pred, average='weighted'))\n",
    "# print(\"Confusion matrix:\\n\", pd.crosstab(y_test, pred, rownames=['True'], \n",
    "# colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": washington by exposure for and economics of in the psychology\n",
      "ALEX BERENSON: these implicitly either them corporation calif rand goldman we make\n",
      "AMY HARMON: months approach toxic less treatment promising breakthrough effective championed results\n",
      "ANDREW JACOBS: her ms coup little platform assert campaign oust rousseff état\n",
      "ANDREW R. CHOW: coachella angus reversed brought singer will guns roses ac dc\n",
      "ANNIE CORREAL: spends son previously alpert edited month andavolu endings obit his\n",
      "BEN KENIGSBERG: portrait hardly radio paints 90 91 wfmu fm film city\n",
      "BENJAMIN WEISER: sporyshev based prisoners television charged inspired was the mr he\n",
      "CEYLAN YEGINSU: journalists quickly istanbul media potential concerns prompting whether turkish closed\n",
      "CHARLES ISHERWOOD: carlyle top thomas cirque concurrently lucas du season timing continuing\n",
      "CHARLES M. BLOW: bash comment odd planned nomination cnn thursday convention if democratic\n",
      "CHOE SANG-HUN: chul dong naturalized steal kim claims koreans had south military\n",
      "CLINTON: namt vas incident collano employ mike himself tree lathrop sawing\n",
      "CNBC: exposure facing of his support research discusses parker immunotherapy sean\n",
      "DEAN R. LEIMER, and SELIG D. LESNOY: support offset impact however recommendations saving negative designed hand does\n",
      "DENISE GRADY: 52 nearly disease cure the 000 breast still strong cancer\n",
      "DWIGHT GARNER: among senior poems poem flowering betjeman novels front writes bring\n",
      "ERIK PIEPENBURG: novelist theater younger female hander someone male company relationship frisky\n",
      "FARAH STOCKMAN: philadelphia away crossing appeared protested rally carrying april his so\n",
      "GARDINER HARRIS: moonshot 2003 initiative barriers biden jr will to cancer her\n",
      "GEORGE R. HARRIS: imitation went immediately fired 77 was the it whiz bang\n",
      "GINA KOLATA: strong officially type patients downgraded decided panel thousands tumor cancer\n",
      "GRAHAM BOWLEY: monday him thrown sexual effort cosby assault pennsylvania appeals blocked\n",
      "INTERNATIONAL HERALD TRIBUNE: defense admitted admission spy germany minister had been west she\n",
      "JAMES PONIEWOZIK: jeffrey garry audience hank preparing hear tambor the sanders larry\n",
      "JAVIER C. HERNÁNDEZ: sentenced documents threats beijing technician china 150 selling combat aggressive\n",
      "JEREMY EGNER: creating led precursor invite performances sorts tonight host show guest\n",
      "JOHN SCHWARTZ: mobil eric investigation schneiderman statements is strong exxon global warming\n",
      "JON PARELES: foot arena brings together festival immobile keep injured reunited founding\n",
      "JON SANDERS FILMS: her loss come is inspirational terms struggling since widow teacher\n",
      "KAREN ALEXANDER: into others virility step gym quest trap unwittingly turbocharged rats\n",
      "KAREN W. ARENSON: lesnoy disclosed calculation paper erred turned flaw the in feldstein\n",
      "KEN JAWOROWSKI: strong writing intriguing saves script moments place the sex partly\n",
      "KJ DELL'ANTONIA: williams starts good mary catastrophes those phone happy elizabeth with\n",
      "LISA SANDERS, M.D: solve readers unexplained gain year old why woman 59 weight\n",
      "MARK MAZZETTI: head 83 lawyer intelligence role leesburg contra career in his\n",
      "MARTIN S. FELDSTEIN: years during problems few increasing decide next financial deal the\n",
      "MICHAEL SCHWIRTZ and MICHAEL WINERIP: with district reviewing attorney jurisdiction inmate violent encounter 2010 wednesday\n",
      "MICHAEL WINERIP, MICHAEL SCHWIRTZ and TOM ROBBINS: brutality northern bad investigations federal murderers unusually agency past pep\n",
      "MIKKAEL A. SEKERES, M.D: say environmental impossible need cancers collective randomly except nose specific\n",
      "NEIL GENZLINGER: isn technologically dementia easier impactful personal coherent thank struggle playing\n",
      "NO POLITICS: jnew 19o3 _____ benefit 22 jan gratify committee mcgillicudy some\n",
      "PATRICK J. EGAN and MEGAN MULLIN: evenly pleasant living period across themselves although seasons counties been\n",
      "PAUL KRUGMAN: to were an as they motivated reasoning antidote models doing\n",
      "PETER KEEPNEWS: ran dark behind comics hbo 1998 stand show sanders larry\n",
      "Pamela G. Hollie: sisters penthouse guccione own begun suspense winston twin will magazine\n",
      "REUTERS: downplayed sarcastically calling win state presidential senator victory achievement trump\n",
      "ROGER COHEN: politics is change planet come the body am therefore home\n",
      "SAMANTH SUBRAMANIAN: survey carry instead secret das became britain enchanted sarat chandra\n",
      "SIMONE GUBLER: administrators sporting daily unarmed events law carve environment guns into\n",
      "STEWART AIN: awakening residents cents consistently gallon areas south gasoline fork prices\n",
      "SYLVIANE GOLD: hartford has arrived laptop clunky antiquated gone matter delightful time\n",
      "Special to The New York Times: coolidge action authorized munitions hughes establishing shipment issued embargo the\n",
      "THE ASSOCIATED PRESS: pellet felony middleweight kelly shooting ohio accused pavlik indicted matt\n",
      "TIMOTHY EGAN: regarding catholics pope apostolic desire stirring teachings exhortation church skeptics\n",
      "WILLIAM B. GAIL: significant longstanding century hence predicted comprehend repeatable yourself scientists occur\n",
      "Wireless to THE NEW YORK TIMES: wireless acclaimed strong by espionage play new london hackett walter\n",
      "YAMICHE ALCINDOR: bill citizens situation unemployed 2000 mrs strong hillary five clinton\n"
     ]
    }
   ],
   "source": [
    "# print ten most important words for each topic\n",
    "show_top10(clf, vectorizer, le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify by author using articles from different time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:  ['economics', 'politics', 'espionage', 'global+warming', 'clinton', 'sanders', 'guns', 'cancer', 'sex']\n",
      "NPages:  [150, 100, 50, 10, 10, 10, 10, 10, 10] \n",
      "\n",
      "Total documents:  3600\n",
      "Started download...\n",
      "economics is done | 150/360\n",
      "politics is done | 250/360\n",
      "espionage is done | 300/360\n",
      "global+warming is done | 310/360\n",
      "clinton is done | 320/360\n",
      "sanders is done | 330/360\n",
      "guns is done | 340/360\n",
      "cancer is done | 350/360\n",
      "sex is done | 360/360\n",
      "\n",
      "Done in  44.43974494934082 seconds\n",
      "{'section_name': {'display_name': 'N.Y. / Region', 'content': 'nyregion'}, 'date_published': '2008-03-30T00:00:00Z', 'user_topic': 'economics', 'keywords': [{'value': 'Prices (Fares, Fees and Rates)', 'rank': '1', 'is_major': 'N', 'name': 'subject'}, {'value': 'Oil (Petroleum) and Gasoline', 'rank': '2', 'is_major': 'N', 'name': 'subject'}, {'value': 'Hamptons (NY)', 'rank': '3', 'is_major': 'N', 'name': 'glocations'}, {'value': 'Roads and Traffic', 'rank': '4', 'is_major': 'N', 'name': 'subject'}], 'weburl': 'http://www.nytimes.com/2008/03/30/nyregion/nyregionspecial2/30gasli.html', 'snippet': 'GASOLINE prices on the South Fork are consistently 20 to 50 cents more per gallon than in other areas of Long Island. But the recent run-up in gasoline prices to record levels has been a rude awakening for some South Fork residents.', 'nytclass': 'Oil (Petroleum) and Gasoline; Prices (Fares, Fees and Rates); Roads and Traffic', 'author': 'STEWART AIN', 'date_modified': '2015-01-26T05:04:19Z', 'abstract': None, 'title': 'Steep Gas Prices on the South Fork Draw More Than a Shock', 'lead_paragraph': 'GASOLINE prices on the South Fork are consistently 20 to 50 cents more per gallon than in other areas of Long Island. But the recent run-up in gasoline prices to record levels has been a rude awakening for some South Fork residents.'}\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "# Classify by author using articles from different time periods\n",
    "articles = get_data(TOPICS = topics, NPAGES = [150,100,50,10,10,10,10,10,10])\n",
    "print(articles[2])\n",
    "# number of articles\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Nones. Good to go.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['decide during the next few years how best to deal with the increasing financial problems of the Social Security program. <strong>ECONOMICS</strong>',\n",
       " \"this time for a theory of economic policy that is appropriate for the 1970's and 80's in much the same way that Keynesian <strong>economics</strong> was a product of the times in the 1920's and 30's. That range of policies survived many shocks from\",\n",
       " 'standards on scaffolding, combined savings. asbestos exposure, cadmium and chromium exposure, and grain elevator dust control <strong>ECONOMICS</strong>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some articles are missing lead paragraph and abstract. just use snippets for now\n",
    "data = [ x['snippet'] for x in articles]\n",
    "\n",
    "# check if data has Nones\n",
    "nonesidx = [data.index(x) for x in data if x == None and len(x) > 0]\n",
    "if len(nonesidx) > 0:\n",
    "    print(\"You have nones. Below are indices of articles.\")\n",
    "    print(nonesidx)\n",
    "else:\n",
    "    print(\"No Nones. Good to go.\")\n",
    "    \n",
    "data[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIMOTHY EGAN', 'KAREN ALEXANDER', 'DWIGHT GARNER', 'BEN KENIGSBERG']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article authors\n",
    "label = [ x['author'] for x in articles]\n",
    "label[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2412, 1249) (1188, 1249)\n"
     ]
    }
   ],
   "source": [
    "vectors = vectorizer.fit_transform(data)\n",
    "target = le.fit_transform(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, target, test_size=0.33, \n",
    "                                                    random_state=1729)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:  0.815594555496\n",
      "Accuracy:  0.791768788424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tpot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/envs/tpot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"F-score: \", metrics.f1_score(y_test, pred, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.precision_score(y_test, pred, average='weighted'))\n",
    "# print(\"Confusion matrix:\\n\", pd.crosstab(y_test, pred, rownames=['True'], \n",
    "# colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.774\n",
      "Completeness: 0.987\n",
      "V-measure: 0.868\n",
      "Adjusted Rand-Index: 0.814\n",
      "Silhouette Coefficient: 0.405\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y_test, pred))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(y_test, pred))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(y_test, pred))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(y_test, pred))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_test, pred, sample_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
